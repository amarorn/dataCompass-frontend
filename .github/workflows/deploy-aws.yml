name: Deploy DataCompass Frontend to AWS EKS

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: whatsapp-analytics-production
  ECR_REPOSITORY: datacompass-frontend
  IMAGE_TAG: ${{ github.sha }}
  NODE_VERSION: '20'

jobs:
  test:
    name: Run Tests and Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: 8
        
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'
        
    - name: Install dependencies
      run: pnpm install
      
    - name: Run linting
      run: pnpm run lint
      
    - name: Run type checking (if TypeScript)
      run: pnpm run type-check || echo "TypeScript not configured"
      
    - name: Run tests
      run: pnpm run test || echo "Tests not configured yet"
      
    - name: Build application
      run: pnpm run build
      
    - name: Check build size
      run: |
        echo "## ðŸ“¦ Build Size Analysis" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        du -sh dist/ >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        find dist/ -name "*.js" -o -name "*.css" | head -10 | xargs ls -lh >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: build-files
        path: dist/
        retention-days: 1

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: 8
        
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'
        
    - name: Install dependencies
      run: pnpm install
      
    - name: Run npm audit
      run: pnpm audit --audit-level moderate || echo "Audit completed with warnings"
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        
    - name: Run Trivy for high/critical vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        severity: 'HIGH,CRITICAL'
        exit-code: '1'

  build-and-push:
    name: Build and Push Docker Image to ECR
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: Create ECR repository if it doesn't exist
      run: |
        aws ecr describe-repositories --repository-names $ECR_REPOSITORY --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository --repository-name $ECR_REPOSITORY --region ${{ env.AWS_REGION }}
        
    - name: Build Docker image
      run: |
        docker build -t $ECR_REPOSITORY:$IMAGE_TAG .
        docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG
        docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:latest
        
    - name: Scan Docker image for vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}'
        format: 'table'
        severity: 'HIGH,CRITICAL'
        exit-code: '0'
        
    - name: Test Docker image
      run: |
        echo "ðŸ§ª Testing Docker image..."
        
        # Clean up any existing containers
        docker rm -f test-container 2>/dev/null || true
        
        # Start container
        echo "Starting container..."
        docker run --rm -d --name test-container -p 8080:8080 $ECR_REPOSITORY:$IMAGE_TAG
        
        # Wait for container to be ready
        echo "Waiting for container to start..."
        sleep 20
        
        # Check if container is still running
        if ! docker ps | grep -q test-container; then
          echo "âŒ Container stopped unexpectedly"
          echo "Container logs:"
          docker logs test-container 2>/dev/null || echo "No logs available"
          exit 1
        fi
        
        # Test if the application is responding
        echo "Testing application response..."
        for i in {1..5}; do
          if curl -f -s http://localhost:8080/ > /tmp/response.html; then
            echo "âœ… Application is responding"
            if grep -q -i "datacompass\|react\|html\|<!DOCTYPE" /tmp/response.html; then
              echo "âœ… Application content looks correct"
              break
            else
              echo "âš ï¸ Response received but content unexpected"
              echo "First 200 chars of response:"
              head -c 200 /tmp/response.html
            fi
          else
            echo "âš ï¸ Attempt $i failed, retrying in 5s..."
            sleep 5
          fi
          
          if [ $i -eq 5 ]; then
            echo "âŒ Application test failed after 5 attempts"
            echo "Container status:"
            docker ps -a | grep test-container || echo "Container not found"
            echo "Container logs:"
            docker logs test-container 2>/dev/null || echo "No logs available"
            docker rm -f test-container 2>/dev/null || true
            exit 1
          fi
        done
        
        echo "ðŸŽ‰ Docker image test completed successfully"
        docker rm -f test-container 2>/dev/null || true
        
    - name: Push image to ECR
      run: |
        docker push ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG
        docker push ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:latest
        
    - name: Update image tag in Kubernetes manifests
      run: |
        sed -i "s|IMAGE_TAG_PLACEHOLDER|${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG|g" k8s/base/deployment.yaml
        
    - name: Upload updated manifests
      uses: actions/upload-artifact@v4
      with:
        name: k8s-manifests
        path: k8s/
        retention-days: 1

  deploy-to-staging:
    name: Deploy to Staging Environment
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download K8s manifests
      uses: actions/download-artifact@v4
      with:
        name: k8s-manifests
        path: k8s/
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Update kubeconfig and verify access
      run: |
        echo "ðŸ”§ Configuring kubectl for EKS cluster..."
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
        echo "ðŸ” Verifying AWS identity..."
        aws sts get-caller-identity
        
        echo "ðŸ” Testing kubectl connectivity..."
        kubectl version --client
        kubectl cluster-info
        
        echo "ðŸ” Testing basic kubectl access..."
        kubectl auth can-i get pods --all-namespaces || echo "âš ï¸ Limited permissions detected"
        
    - name: Create namespace if not exists
      run: |
        echo "ðŸ“¦ Creating staging namespace..."
        kubectl create namespace staging --dry-run=client -o yaml | kubectl apply -f - || {
          echo "âŒ Failed to create namespace. Checking cluster access..."
          kubectl get namespaces || echo "âŒ Cannot list namespaces"
          kubectl auth can-i create namespaces || echo "âŒ No permission to create namespaces"
          exit 1
        }
        
    - name: Deploy to staging using Kustomize
      run: |
        # Update image tag in kustomization
        cd k8s/overlays/staging
        kustomize edit set image datacompass-frontend=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}
        
        # Apply manifests
        kubectl apply -k . -n staging
        
        # Wait for deployment
        kubectl rollout status deployment/datacompass-frontend -n staging --timeout=300s
        
    - name: Run staging smoke tests
      run: |
        echo "ðŸ§ª Running staging smoke tests..."
        
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=datacompass-frontend -n staging --timeout=300s
        
        # Get service URL
        SERVICE_URL=$(kubectl get service datacompass-frontend-service -n staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        
        if [ -z "$SERVICE_URL" ]; then
          echo "âš ï¸ Load balancer not ready, using port-forward for testing"
          kubectl port-forward service/datacompass-frontend-service 8080:80 -n staging &
          sleep 10
          SERVICE_URL="localhost:8080"
        fi
        
        # Wait for service to be ready
        sleep 30
        
        # Run health checks
        curl -f http://$SERVICE_URL/health || echo "âš ï¸ Health check failed, but continuing"
        curl -f http://$SERVICE_URL/ || echo "âš ï¸ Main page check failed, but continuing"
        
        echo "âœ… Staging deployment completed!"
        
    - name: Get staging URL
      run: |
        INGRESS_URL=$(kubectl get ingress datacompass-frontend-ingress -n staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
        echo "## ðŸš€ Staging Deployment" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Environment**: Staging" >> $GITHUB_STEP_SUMMARY
        echo "- **Image**: ${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}" >> $GITHUB_STEP_SUMMARY
        echo "- **URL**: http://$INGRESS_URL" >> $GITHUB_STEP_SUMMARY
        echo "- **Namespace**: staging" >> $GITHUB_STEP_SUMMARY

  deploy-to-production:
    name: Deploy to Production Environment
    runs-on: ubuntu-latest
    needs: deploy-to-staging
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download K8s manifests
      uses: actions/download-artifact@v4
      with:
        name: k8s-manifests
        path: k8s/
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Update kubeconfig and verify access
      run: |
        echo "ðŸ”§ Configuring kubectl for EKS cluster..."
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
        echo "ðŸ” Verifying AWS identity..."
        aws sts get-caller-identity
        
        echo "ðŸ” Testing kubectl connectivity..."
        kubectl version --client
        kubectl cluster-info
        
        echo "ðŸ” Testing basic kubectl access..."
        kubectl auth can-i get pods --all-namespaces || echo "âš ï¸ Limited permissions detected"
        
    - name: Create namespace if not exists
      run: |
        echo "ðŸ“¦ Creating production namespace..."
        kubectl create namespace production --dry-run=client -o yaml | kubectl apply -f - || {
          echo "âŒ Failed to create namespace. Checking cluster access..."
          kubectl get namespaces || echo "âŒ Cannot list namespaces"
          kubectl auth can-i create namespaces || echo "âŒ No permission to create namespaces"
          exit 1
        }
        
    - name: Deploy to production using Kustomize
      run: |
        # Update image tag in kustomization
        cd k8s/overlays/production
        kustomize edit set image datacompass-frontend=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}
        
        # Apply manifests
        kubectl apply -k . -n production
        
        # Wait for deployment with longer timeout for production
        kubectl rollout status deployment/datacompass-frontend -n production --timeout=600s
        
    - name: Run production health checks
      run: |
        echo "ðŸš€ Running production health checks..."
        
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=datacompass-frontend -n production --timeout=300s
        
        # Get service URL
        SERVICE_URL=$(kubectl get service datacompass-frontend-service -n production -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        
        if [ -z "$SERVICE_URL" ]; then
          echo "âš ï¸ Load balancer not ready, using port-forward for testing"
          kubectl port-forward service/datacompass-frontend-service 8080:80 -n production &
          sleep 10
          SERVICE_URL="localhost:8080"
        fi
        
        # Wait for service to be ready
        sleep 60
        
        # Run comprehensive health checks
        curl -f http://$SERVICE_URL/health || echo "âš ï¸ Health check failed"
        curl -f http://$SERVICE_URL/ || echo "âš ï¸ Main page check failed"
        
        # Test API proxy (if backend is available)
        curl -f http://$SERVICE_URL/api/health || echo "âš ï¸ API proxy check failed (backend may not be ready)"
        
        echo "ðŸš€ Production deployment completed!"
        
    - name: Get production URL and notify
      run: |
        INGRESS_URL=$(kubectl get ingress datacompass-frontend-ingress -n production -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
        
        echo "## ðŸŽ‰ Production Deployment Successful!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Environment**: Production" >> $GITHUB_STEP_SUMMARY
        echo "- **Image**: ${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}" >> $GITHUB_STEP_SUMMARY
        echo "- **URL**: https://$INGRESS_URL" >> $GITHUB_STEP_SUMMARY
        echo "- **Namespace**: production" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”— Quick Links" >> $GITHUB_STEP_SUMMARY
        echo "- [Application](https://$INGRESS_URL)" >> $GITHUB_STEP_SUMMARY
        echo "- [Health Check](https://$INGRESS_URL/health)" >> $GITHUB_STEP_SUMMARY
        echo "- [Kubernetes Dashboard](https://console.aws.amazon.com/eks/home?region=${{ env.AWS_REGION }}#/clusters/${{ env.EKS_CLUSTER_NAME }})" >> $GITHUB_STEP_SUMMARY
        
    - name: Notify deployment failure and rollback
      if: failure()
      run: |
        echo "âŒ Production deployment failed!" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”„ Attempting rollback to previous version..." >> $GITHUB_STEP_SUMMARY
        kubectl rollout undo deployment/datacompass-frontend -n production || echo "Rollback attempted"

  notify:
    name: Notify Deployment Results
    runs-on: ubuntu-latest
    needs: [deploy-to-production]
    if: always()
    
    steps:
    - name: Notify Slack on Success
      if: success() && vars.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#deployments'
        fields: repo,message,commit,author,action,eventName,ref,workflow
        text: |
          ðŸŽ‰ DataCompass Frontend deployed successfully to production!
          ðŸ”— Repository: ${{ github.repository }}
          ðŸ“ Commit: ${{ github.sha }}
          ðŸŒ¿ Branch: ${{ github.ref_name }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
    - name: Notify Slack on Failure
      if: failure() && vars.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#deployments'
        fields: repo,message,commit,author,action,eventName,ref,workflow
        text: |
          âŒ DataCompass Frontend deployment failed!
          ðŸ”— Repository: ${{ github.repository }}
          ðŸ“ Commit: ${{ github.sha }}
          ðŸŒ¿ Branch: ${{ github.ref_name }}
          ðŸ”„ Please check the logs and consider rollback if needed.
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
    - name: Final Summary
      run: |
        echo "## ðŸ“Š Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Triggered by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Event**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY

